# Benchmarking infrastructure design

This documents outlines the design of the infrastructure to be build for ZK WASM benchmarking.

 The insights we want to gain with benchmarking can be divided into the following two categories:

- **Analysis** targets understanding the performance of zkASM generated by the compiler and tries to answer questions like: Which instructions are executed how often during execution and which are hot instructions? How is performance affected by changes to the compiler? How can the performance of generated zkASM be improved?
- **Comparison** relates the performance of zk wasm to the performance of other zk proving systems and tries to answer questions like: Which proving system is more performant? What are the reasons for differences in performance?

# Analysis

This section describes the technical architecture and user interface of infrastructure and tooling related to *analysis*.

## Instrumentation

Instrumentation is based on logging zkASM instructions by calling javascript from zkASM. Actually not zkASM is executed but PIL, which is why it is required to go via javascript. Code for zkasm instruction logging is inserted into compiled zkASM in post processing.

Subsequently raw instruction logs are processed to make them more readable and surface important information. It should tell how often each instruction was executed and how often registers were allocated. Information to be aggregated includes the number of times an instruction was executed and the number of register allocations. Analyzing the number of register allocations is relevant as [this example](https://github.com/near/wasmtime/issues/181) shows how reducing register allocations can decrease the number of cycles.

## Visualization

Once the data described in the previous section is available and has been used while working on the repo, opportunities for visualizations might be identified. They should help to make the information more easily digestible. Some examples of possible visualizations are:

- The number of times instructions were executed.
- zkASM instructions and the number of cycles required for their execution.
- The number of cycles required for benchmarks across different points in the git commit history.

## Relating zkASM instructions to cycles

Optimizations ultimately aim at reducing the number of cycles required to generate a proof. Hence instrumentation should tie zkASM instructions to the number of cycles required to execute these instructions. This can help to identify the most promising approaches to optimizations.

## Units of measurement

For optimizing zkASM produced by the compiler, we can look at the number of cycles of the virtual machine. This has the advantage of being deterministic, as it does not depend on the physical hardware and load of the machine that executes the program. For this reasons, statistical approaches like sampling are not required.

Ultimately, however, wall clock time still is an important factor for zk wasm use cases. Therefore it would be helpful to establish some relation between vm cycles and wall clock time on some standardized hardware.

## User interface

There should be single command that allows to compare the current code (e.g. after making changes) to committed numbers. Developers can run this command during development. It should be re-used in CI to make CI outcomes predictable once a command has been run locally.

### Customizable behavior

The default analysis should be customizable via command line flags, to allow drilling down on some performance characteristics. There is help command that list and explains these flags, so user of the benchmarking tools don’t have to read the source code to tweak its behavior. The output of the help command should be auto-generated from source code to avoid it becomes out of sync with the tool’s actual behavior.

### Output data format

There should be the following two formats for output data: First, a pretty print format intended to be read by humans during development. Second, a standardized and (de)serializable data format like JSON or YAML that can be used in combination with other tools. The desired output format can be selected with a command line flag.

## Integration with CI

As it should be possible to run the default benchmarking with a single command, it should be easy to integrate that into CI. For PRs, deviations from the base branch could be posted in the conversation by a bot. This allows reviewers to immediately see the impact of changes on zkASM performance, without having to check-out the PR’s branch and run benchmarking locally.

# Comparison

Comparison of zk wasm with other zk system allows to evaluate its relative efficiency. Zk systems suitable for comparison are risc-0 and polygon miden.

## Comparable benchmarks

The other systems publish benchmarks for some common computations like hash computations. To make zk wasm comparable with these systems, we should run the same benchmarks. Note that initially it might not be possible to compile Rust programs doing these computations as currently not all wasm instructions are supported. Programs for these benchmark computations will be added gradually.

## Variable job size

Benchmarks for other zk systems have variable job sizes, for instance different byte sizes of data to be hashed. It is interesting to see how performance develops as the job size increases. For comparison, zk wasm benchmarks should support variable job size as well.

how to read external input?

## Different hardware

Some benchmarks of other zk systems are run on different hardware. Zk wasm might run benchmarks on the hardware used by other systems as well to allow for a more complete comparison. This work is lower priority and rather unlikely to be realized in the first iteration of the benchmarking infrastructure.

## Visualization

Once the data described in the previous section is available and has been used while working on the repo, opportunities for visualizations might be identified. They should help to make the information more easily digestible. Some examples of possible visualizations are:

- The time required for jobs as a function of the job size.
- The time required for jobs by different ZK systems.
- The time required for jobs across different git commits.

## Developer experience

There should be a single command to run all benchmarks and compare them to other zk systems. Ideally the command used locally by developers can be re-used by CI to make the result of CI runs predictable. It should be possible to easily filter the set of benchmarked computations using command line flags.

### Customizable behavior

Command line flags should be documented and ideally that documentation is automatically generated from doc comments in the code. This helps to avoid documentation becoming outdated and should allow developers to customize *Comparison* benchmarking without the need to study source code.

### Output data format

There should be the following two formats for output data: First, a pretty print format intended to be read by humans during development. Second, a standardized and (de)serializable data format like JSON or YAML that can be used in combination with other tools. The desired output format can be selected with a command line flag.

## Integration with CI

As it should be possible to run the default benchmarking with a single command, it should be easy to integrate that into CI. For PRs, deviations from the base branch could be posted in the conversation by a bot. This allows reviewers to immediately see the impact of changes on zkASM performance related to other ZK systems, without having to check-out the PR’s branch and run benchmarking locally.

# Timeline

Within 5 weeks the benchmarking infrastructure should be set up and allow retrieving the basic information required to answer questions related to *Analysis* and *Comparison*. While working on and using benchmarking tools, usage patterns should emerge. The following 3 weeks can than be used to improve developer experience and CI integration. The following table summarizes the timeline.


| Milestone | Duration | Implements                                                   |
|-----------|----------|--------------------------------------------------------------|
| 1         | ~5 weeks | Provide data to answer questions related to *Analysis* and *Comparison*. |
| 2         | ~3 weeks | Improve developer experience, support more customizations, and provide visualizations. |

# Other features

This section lists features that are out of scope for Stage 2 (link gh issue) but might be implemented later on:

- Enable tracing back from zkASM instructions to the wasm instructions that triggered their emission.
- Running *Comparison* benchmarks on different hardware.
- Benchmarking of computations inside a wasm interpreter compiled to zkASM.
