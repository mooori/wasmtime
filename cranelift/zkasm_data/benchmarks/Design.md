# Benchmarking infrastructure design

This documents outlines the design of the infrastructure to be build for ZK WASM benchmarking. The insights we want to gain with benchmarking can be divided into the following two categories:

- **Analysis** targets understanding the performance of zkASM generated by the compiler and tries to answer questions like: Which instructions are executed how often during execution and which are hot instructions? How is performance affected by changes to the compiler? How can the performance of generated zkASM be improved?
- **Comparison** relates the performance of ZK WASM to the performance of other ZK proving systems and tries to answer questions like: Which proving system is more performant? What are the reasons for differences in performance?

# Analysis

This section describes the technical architecture and user interface of infrastructure and tooling related to *analysis*.

## Instrumentation

Instrumentation is based on logging zkASM instructions by calling Javascript from zkASM. The indirection via Javascript is required as `zkevm-proverjs` executes compiled PIL instead of zkASM. The code for zkASM instruction logging might be inserted to zkASM by enabling a compilation flag or in post processing.

Subsequently raw instruction logs are processed to aggregate data and surface important information. Information to be aggregated includes the number of times an instruction was executed and the number of register allocations. Analyzing the number of register allocations is relevant as [this example](https://github.com/near/wasmtime/issues/181) shows how reducing register allocations can decrease the number of cycles.

TODO specify data to be logged after we committed to the visualization scheme.

## Visualization

Visualizations should help to make the data described above more easily comprehensible and highlight the most expensive opcodes. The following graph shows the schema of the visualizations to be produced for *Analysis*:

![Analysis graph](/cranelift/zkasm_data/benchmarks/assets/analysis_graph.svg)

The graph represents an ordered list of the program's [`MInst`], which can be found on the bottom. Above each `MInst` are the zkASM instructions that have been executed due to its lowering. The following characteristics should hold for the visualization:

- Ordered by decreasing `<count>` from left to right.
- `<count>` is defined as a tuple `(a, b, c)` of the elements `{num_occurrences, num_cycles, num_reg_writes}`.
    - `num_occurrences` is the number of times the instruction was executed.
    - `num_cycles` is the total number of virtual machine cycles required to execute the instruction.
    - `num_reg_writes` is the total number of register writes caused by the execution of the instruction.
    - For `zkasm_op_x` sums are taken separately for each `MInst` that emitted `zkasm_op_x`.
- The order of the tuple elements can be chosen via an UI or CLI flags. For example, `num_cycles` can be shown first, in which case `a = num_cycles`.
- Sorting is done by comparing the first element of `<count>` tuples.
- All `zkasm_op_*` rectangles have the same width.
- The width of each `MInst::*` rectangle is determined by the number of `zkasm_op_*` rectangles on top of it.

Due to the sort order defined above, there are different graphs for different assignments of `a`. This allows developers to highlight different costs, for instance register writes if `a = num_reg_writes` or virtual machine cycles if `a = num_cycles`.

Once the data described in the previous section is available and has been used while working on the repository, opportunities for visualizations might be identified. They should help to make the information more easily digestible. Some examples of possible visualizations are:

### Format

If feasible, visualizations should be SVG files, again taking inspiration from flame graphs. To avoid cluttering the graph, `<count>` is initially hidden and revealed for a rectangle on hovering.

Producing the graph as SVG file provides the following advantages: 

- No dependencies required as SVG files can be viewed in a web browser.
- They can be embedded in Github comments, markdown files and other documents.

### Diff visualization

It will be interesting to observe how changes to generated zkASM impact performance. Given the *Analysis* data for two different commits `C1` and `C2`, above visualization can be used for displaying diffs. The diff data can be calculated as `<count_x_C1> - <count_x_C2` for `MInst` and zkASM instructions. The graph based on diff data then helps to identify the instructions whose cost changes most between the commits `C1` and `C2`.

## Relating zkASM instructions to cycles

Optimizations ultimately aim at reducing the number of cycles required to generate a proof. Hence instrumentation should tie zkASM instructions to the number of cycles required to execute these instructions. This can help to identify the most promising approaches to optimization.

## Units of measurement

For optimizing zkASM produced by the compiler, we can look at the number of cycles of the virtual machine. This has the advantage of being deterministic, as it does not depend on the hardware and load of the machine that executes the program. For these reasons, statistical approaches like sampling are not required.

Ultimately, however, wall clock time still is an important factor for ZK WASM use cases. Therefore it would be helpful to establish some relation between virtual machine cycles and wall clock time on standardized hardware.

## User interface

There should be a single command that allows to generate the data described above. This command can be run during development to assess the performance impact of changes. The same command should be used in CI to make CI outcomes predictable once the command has been run locally.

In case a snapshot of data is committed to the repository, a comparison of data at different commits can be enabled optionally.

### Customizable behavior

The default analysis should be customizable via command line flags, to allow drilling down on some performance characteristics. There is a help command that lists and explains these flags, so users of the benchmarking tools don’t have to read the source code to tweak its behavior. The output of the help command should be auto-generated from source code to avoid it becoming out of sync with the tools’ actual behavior.

### Output data format

There should be the following two formats for output data: First, a pretty print format intended to be read by humans during development. Second, a standardized and (de)serializable data format like JSON or YAML that can be used in combination with other tools. The desired output format can be selected with a command line flag.

## Integration with CI

As it should be possible to run the default benchmarking analysis with a single command, it should be easy to integrate it into CI. For PRs, deviations from the base branch could be posted in the conversation by a bot in case data snapshots are committed to the repository. This would allow reviewers to immediately see the impact of changes on zkASM performance, without having to check-out the PR’s branch and run benchmarking locally.

# Comparison

*Comparison* of ZK WASM with other ZK systems allows to evaluate its relative efficiency. ZK systems suitable for comparison are [RISC Zero](https://www.risczero.com) and [Polygon Miden](https://polygon.technology/polygon-miden).

## Comparable benchmarks

The ZK systems mentioned above publish benchmarks for some common computations like hashing [[1](https://risc0.github.io/ghpages/dev/benchmarks/index.html), [2](https://github.com/0xPolygonMiden/miden-vm#performance)]. To make ZK WASM comparable with these systems, we should run the same benchmarks. Note that initially it might not be possible to compile Rust programs doing these computations as currently not all wasm instructions are supported. Therefore, programs for these benchmark computations will be added gradually.

## Variable job size

Benchmarks for other ZK systems have variable job sizes, for instance different sizes of data to be hashed. It is interesting to see how performance develops as the job size increases. For comparison, ZK WASM benchmarks should support variable job size as well.

## Different hardware

Some benchmarks of other ZK systems are run on different hardware. ZK WASM might run benchmarks on the hardware used by other systems as well to allow for a more complete comparison. This work is lower priority and rather unlikely to be realized in the first iteration on the benchmarking infrastructure.

## Visualization

Once the data described in the previous section is available and has been used while working on the repository, opportunities for visualizations might be identified. They should help to make the information more easily digestible. Some examples of possible visualizations are:

- The time required for jobs as a function of the job size.
- The time required for jobs by different ZK systems.
- The time required for jobs across different git commits.

## Developer experience

There should be a single command to run all benchmarks and compare performance to other ZK systems. Ideally the command used locally by developers can be re-used by CI to make the result of CI runs predictable.

It should be possible to easily filter the set of benchmarked computations using command line flags. For example, during local development it should be possible to disable long running benchmarks with large job size.

### Customizable behavior

Command line flags should be documented and ideally that documentation is automatically generated from doc comments in the code. This helps to avoid documentation becoming outdated and should allow developers to customize *Comparison* benchmarking without the need to study source code.

### Output data format

There should be the following two formats for output data: First, a pretty print format intended to be read by humans during development. Second, a standardized and (de)serializable data format like JSON or YAML that can be used in combination with other tools. The desired output format can be selected with a command line flag.

## Integration with CI

As it should be possible to run the default benchmarking with a single command, it should be easy to integrate it into CI. For PRs, deviations from the base branch could be posted in the conversation by a bot in case data snapshots are committed to the repository. This would allow reviewers to immediately see the impact of changes on zkASM performance relative to other ZK systems, without having to check-out the PR’s branch and run benchmarking locally.

# Timeline

Within 5 weeks the benchmarking infrastructure should be set up and allow retrieving the basic information required to answer questions related to *Analysis* and *Comparison*. While working on and using benchmarking tools, usage patterns should emerge. The following 3 weeks can then be used to improve developer experience and CI integration. The following table summarizes the timeline.


| Milestone | Duration | Implements                                                   |
|-----------|----------|--------------------------------------------------------------|
| 1         | ~5 weeks | Provide data to answer questions related to *Analysis* and *Comparison*. |
| 2         | ~3 weeks | Improve developer experience, support more customizations, and provide visualizations. |

# Other features

This section lists features that are out of scope for [Stage 2](https://github.com/near/wasmtime/blob/main/docs/zkasm/roadmap.md#stage-2) but might be implemented later on:

- Enable tracing back from zkASM instructions to the wasm instructions that triggered their emission.
- Running *Comparison* benchmarks on different hardware.
- Benchmarking of computations inside a wasm interpreter compiled to zkASM.
