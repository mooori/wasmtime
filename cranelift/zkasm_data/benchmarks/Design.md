# Benchmarking infrastructure design

This documents outlines the design of the infrastructure to be build for ZK WASM benchmarking. The insights we want to gain with benchmarking can be divided into the following two categories:

- **Analysis** targets understanding the performance of zkASM generated by the compiler and tries to answer questions like: Which instructions are executed how often during execution and which are hot instructions? How is performance affected by changes to the compiler? How can the performance of generated zkASM be improved?
- **Comparison** relates the performance of ZK WASM to the performance of other ZK proving systems and tries to answer questions like: Which proving system is more performant? What are the reasons for differences in performance?

# Analysis

This section describes the technical architecture and user interface of infrastructure and tooling related to *analysis*.

## Instrumentation

Instrumentation is based on logging zkASM instructions by calling Javascript from zkASM. The indirection via Javascript is required as `zkevm-proverjs` executes compiled PIL instead of zkASM. The code for zkASM instruction logging might be inserted to zkASM by enabling a compilation flag or in post processing.

Subsequently raw instruction logs are processed to aggregate data and surface important information. Information to be aggregated includes the number of times an instruction was executed and the number of register allocations. Analyzing the number of register allocations is relevant as [this example](https://github.com/near/wasmtime/issues/181) shows how reducing register allocations can decrease the number of cycles.

## Visualization

Once the data described in the previous section is available and has been used while working on the repository, opportunities for visualizations might be identified. They should help to make the information more easily digestible. Some examples of possible visualizations are:

- The number of times instructions were executed.
- zkASM instructions and the number of cycles required for their execution.
- The number of cycles required for benchmarks across different points in the git commit history.

## Relating zkASM instructions to cycles

Optimizations ultimately aim at reducing the number of cycles required to generate a proof. Hence instrumentation should tie zkASM instructions to the number of cycles required to execute these instructions. This can help to identify the most promising approaches to optimization.

## Units of measurement

For optimizing zkASM produced by the compiler, we can look at the number of cycles of the virtual machine. This has the advantage of being deterministic, as it does not depend on the hardware and load of the machine that executes the program. For these reasons, statistical approaches like sampling are not required.

Ultimately, however, wall clock time still is an important factor for ZK WASM use cases. Therefore it would be helpful to establish some relation between virtual machine cycles and wall clock time on standardized hardware.

## User interface

There should be a single command that allows to generate the data described above. This command can be run during development to assess the performance impact of changes. The same command should be used in CI to make CI outcomes predictable once the command has been run locally.

In case a snapshot of data is committed to the repository, a comparison of data at different commits can be enabled optionally.

### Customizable behavior

The default analysis should be customizable via command line flags, to allow drilling down on some performance characteristics. There is a help command that lists and explains these flags, so users of the benchmarking tools don’t have to read the source code to tweak its behavior. The output of the help command should be auto-generated from source code to avoid it becoming out of sync with the tools’ actual behavior.

### Output data format

There should be the following two formats for output data: First, a pretty print format intended to be read by humans during development. Second, a standardized and (de)serializable data format like JSON or YAML that can be used in combination with other tools. The desired output format can be selected with a command line flag.

## Integration with CI

As it should be possible to run the default benchmarking analysis with a single command, it should be easy to integrate it into CI. For PRs, deviations from the base branch could be posted in the conversation by a bot in case data snapshots are committed to the repository. This would allow reviewers to immediately see the impact of changes on zkASM performance, without having to check-out the PR’s branch and run benchmarking locally.

# Comparison

*Comparison* of ZK WASM with other ZK systems allows to evaluate its relative efficiency. ZK systems suitable for comparison are [RISC Zero](https://www.risczero.com) and [Polygon Miden](https://polygon.technology/polygon-miden).

## Comparable benchmarks

The ZK systems mentioned above publish benchmarks for some common computations like hashing [[1](https://risc0.github.io/ghpages/dev/benchmarks/index.html), [2](https://github.com/0xPolygonMiden/miden-vm#performance)]. To make ZK WASM comparable with these systems, we should run the same benchmarks. Note that initially it might not be possible to compile Rust programs doing these computations as currently not all wasm instructions are supported. Therefore, programs for these benchmark computations will be added gradually.

## Variable job size

Benchmarks for other ZK systems have variable job sizes, for instance different sizes of data to be hashed. It is interesting to see how performance develops as the job size increases. For comparison, ZK WASM benchmarks should support variable job size as well.

## Different hardware

Some benchmarks of other ZK systems are run on different hardware. ZK WASM might run benchmarks on the hardware used by other systems as well to allow for a more complete comparison. This work is lower priority and rather unlikely to be realized in the first iteration on the benchmarking infrastructure.

## Visualization

Once the data described in the previous section is available and has been used while working on the repository, opportunities for visualizations might be identified. They should help to make the information more easily digestible. Some examples of possible visualizations are:

- The time required for jobs as a function of the job size.
- The time required for jobs by different ZK systems.
- The time required for jobs across different git commits.

## Developer experience

There should be a single command to run all benchmarks and compare performance to other ZK systems. Ideally the command used locally by developers can be re-used by CI to make the result of CI runs predictable.

It should be possible to easily filter the set of benchmarked computations using command line flags. For example, during local development it should be possible to disable long running benchmarks with large job size.

### Customizable behavior

Command line flags should be documented and ideally that documentation is automatically generated from doc comments in the code. This helps to avoid documentation becoming outdated and should allow developers to customize *Comparison* benchmarking without the need to study source code.

### Output data format

There should be the following two formats for output data: First, a pretty print format intended to be read by humans during development. Second, a standardized and (de)serializable data format like JSON or YAML that can be used in combination with other tools. The desired output format can be selected with a command line flag.

## Integration with CI

As it should be possible to run the default benchmarking with a single command, it should be easy to integrate it into CI. For PRs, deviations from the base branch could be posted in the conversation by a bot in case data snapshots are committed to the repository. This would allow reviewers to immediately see the impact of changes on zkASM performance relative to other ZK systems, without having to check-out the PR’s branch and run benchmarking locally.

# Timeline

Within 5 weeks the benchmarking infrastructure should be set up and allow retrieving the basic information required to answer questions related to *Analysis* and *Comparison*. While working on and using benchmarking tools, usage patterns should emerge. The following 3 weeks can then be used to improve developer experience and CI integration. The following table summarizes the timeline.


| Milestone | Duration | Implements                                                   |
|-----------|----------|--------------------------------------------------------------|
| 1         | ~5 weeks | Provide data to answer questions related to *Analysis* and *Comparison*. |
| 2         | ~3 weeks | Improve developer experience, support more customizations, and provide visualizations. |

# Other features

This section lists features that are out of scope for [Stage 2](https://github.com/near/wasmtime/blob/main/docs/zkasm/roadmap.md#stage-2) but might be implemented later on:

- Enable tracing back from zkASM instructions to the wasm instructions that triggered their emission.
- Running *Comparison* benchmarks on different hardware.
- Benchmarking of computations inside a wasm interpreter compiled to zkASM.
